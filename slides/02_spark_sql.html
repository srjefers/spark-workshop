<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">

    <title>Apache Spark 2 Workshop :: Spark SQL</title>

    <meta name="description" content="Apache Spark 2 Workshop :: Spark SQL">
    <meta name="author" content="Jacek Laskowski">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/beige.css" id="theme">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!-- https://github.com/hakimel/reveal.js/issues/174 -->
    <style>
      .slides .header {
        position: absolute;
        top: 0px;
        right: 0px;
      }
      .slides .footer {
        position: absolute;
        bottom: 0px;
        left: 35%;
      }
    </style>
    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="reveal">

      <div class="slides">
        <div class="footer">
          <small>Copyright ©2017 Jacek Laskowski</small>
        </div>

        <section class="intro" data-transition="zoom">
          <p>
            <img width="5%" style="background:none; border:none; box-shadow:none;" data-src="images/scala-logo.png">
            <img width="17%" style="background:none; border:none; box-shadow:none;" data-src="images/spark-logo.png">
            <img width="8%" src="images/jacek_laskowski_20141201_512px.png" style="border: 0">
          </p>
          <h1>Spark SQL</h1>
          <h2>Apache Spark 2</h2>
          <h4><a href="https://twitter.com/jaceklaskowski">@jaceklaskowski</a> / <a href="http://stackoverflow.com/users/1305344/jacek-laskowski">StackOverflow</a> / <a href="https://github.com/jaceklaskowski">GitHub</a><br />
          Books: <a href="http://bit.ly/mastering-apache-spark">Mastering Apache Spark</a>  / <a href="http://bit.ly/spark-structured-streaming">Spark Structured Streaming</a></h4>
        </section>

        <section id="spark-sql">
          <section>
            <h2>Introduction to Spark SQL</h2>
            <ol>
              <li>Spark SQL = <b>SQL</b> + <b>Dataset</b> (and <b>DataFrame</b>)
                <ul>
                  <li>Good ol' <b>SQL</b> statements</li>
                  <li><b>Dataset</b></i> (and <b>DataFrame</b>) data abstractions</li>
                  <li><b>Encoder</b></i> for storage- and performance optimizations</li>
                </ul>
              </li>
              <li>Switch to Mastering Apache Spark 2
                <ul>
                  <li><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql.html">Spark SQL — Structured Queries on Large Scale</a></li>
                </ul>
              </li>
            </ol>
          </section>

          <section id="sparksession">
            <h2>SparkSession</h2>
            <ol>
              <li>The entry point to Spark SQL
                <ul>
                  <li>&hellip;and Spark in general these days</li>
                </ul>
              </li>
              <li>spark-shell gives you one instance as <b>spark</b></li>
              <li>Use <b>SparkSession.builder</b> Fluent API</li>
              <li>Helps creating local Datasets
                <ul>
                  <li><b>spark.range(numberOfRecords)</b></li>
                </ul>
              </li>
              <li>Switch to Mastering Apache Spark 2
                <ul>
                  <li><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-SparkSession.html">SparkSession — The Entry Point to Spark SQL</a></li>
                </ul>
              </li>
            </ol>
          </section>

          <section>
            <h2>DataSource API &mdash; Reading and Writing Datasets</h2>
            <ol>
              <li>Loading Datasets using <b>spark.read</b></li>
              <li>Writing Datasets using <b>dataset.write</b>
                <ul>
                  <li>Lower-case to denote instances (not types)</li>
                </ul>
              </li>
              <li>Switch to Mastering Apache Spark 2
                <ul>
                  <li><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-datasource-api.html">DataSource API — Loading and Saving Datasets</a></li>
                </ul>
              </li>
            </ol>
          </section>

          <section>
            <h2>Reading/Loading Datasets</h2>
            <ol>
              <li><b>spark.read</b> &mdash; DataFrameReader
                <ol>
                  <li><b>format</b></li>
                  <li><b>option</b> and <b>options</b></li>
                  <li><b>schema</b></li>
                  <li><b>load</b></li>
                </ol>
              </li>
            </ol>
            <pre><code>
          val dataset = spark.read.format("csv").load("csvs/*")
            </code></pre>
            <ul>
              <li>Switch to Mastering Apache Spark 2
                <ul>
                  <li><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-DataFrameReader.html">DataFrameReader</a></li>
                </ul>
              </li>
            </ul>
          </section>
          <section>
            <h2>Built-In Formats</h2>
            <ol>
              <li><b>csv</b></li>
              <li><b>jdbc</b></li>
              <li><b>json</b></li>
              <li><b>orc</b></li>
              <li><b>parquet</b></li>
              <li><b>table</b></li>
              <li><b>text</b> and <b>textFile</b></li>
              <li><b>spark.read.format(source: String)</b></li>
            </ol>
          </section>
          <section>
            <h2>Writing/Saving Datasets</h2>
            <ol>
              <li><b>DataFrame.write</b> &mdash; DataFrameWriter
                <ol>
                  <li><b>format</b></li>
                  <li><b>mode</b></li>
                  <li><b>option</b> and <b>options</b></li>
                  <li><b>bucketBy</b> and <b>partitionBy</b></li>
                  <li><b>sortBy</b></li>
                  <li><b>insertInto</b>, <b>save</b> and <b>saveAsTable</b></li>
                </ol>
              </li>
            </ol>
            <pre><code>
              dataset.write.format("json").save("jsons")
            </code></pre>
            <ul>
              <li>Switch to Mastering Apache Spark 2
                <ul>
                  <li><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-DataFrameWriter.html">DataFrameWriter</a></li>
                </ul>
              </li>
            </ul>
          </section>
          <section>
            <h2>Ad-hoc Local Datasets</h2>
            <ol>
              <li><b>Seq(...).toDF("col1", "col2", ...)</b> for local DataFrames</li>
              <li><b>Seq(...).toDS</b> for local Datasets</li>
              <li>Use <b>import spark.implicits._</b></li>
              <li>All Scala Collections supported (almost)</li>
              <li>Switch to Mastering Apache Spark 2
                <ul>
                  <li><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-SparkSession.html#implicits">Implicit Conversions &mdash; implicits object</a></li>
                </ul>
              </li>
            </ol>
          </section>
          <section>
            <h2>Schema</h2>
            <ol>
              <li>Schema = <b>StructType</b> with one or many <b>StructFields</b></li>
              <li>Implicit (inferred) or explicit</li>
              <li><b>dataset.printSchema</b></li>
              <li>Schema is your case class(es)
                <pre><code>
  case class Person(id: Long, name: String)

  import org.apache.spark.sql.Encoders
  val schema = Encoders.product[Person].schema
                </code></pre>
              </li>
              <li>Switch to Mastering Apache Spark 2
                <ul>
                  <li><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-schema.html">Schema — Structure of Data</a></li>
                </ul>
              </li>
            </ol>
          </section>
        </section>

        <section>
          <section>
            <h2>Demo: Creating Spark SQL Application</h2>
            <ol>
              <li>Use <b>IntelliJ IDEA</b> and <b>sbt</b></li>
              <li>Define Spark SQL dependency in <b>build.sbt</b>
                <ul>
                  <li><b>libraryDependencies</b></li>
                </ul>
              </li>
              <li>Write your <b>Spark SQL code</b>
                <ul>
                  <li><b>spark.version</b></li>
                </ul>
              </li>
              <li>Execute <b>sbt package</b></li>
              <li>Run the application using <b>spark-submit</b></li>
            </ol>
          </section>
          <section id="exercise-first-app" data-markdown>
            <script type="text/template">
              ## Exercise: My First Spark SQL Application
              #### spark.version

              1. Create a Scala sbt-managed standalone application
                * Use <b>IntelliJ IDEA</b>
              1. Define Spark SQL dependency in <b>build.sbt</b>
                * **libraryDependencies**
                * **"org.apache.spark" %% "spark-sql" % "2.1.1"**
              1. Write your <b>Spark SQL code</b>, i.e. <b>spark.version</b>
                * **SparkSession**
              1. Execute <b>sbt package</b>
              1. Run the application using <b>spark-submit</b>
              1. Time: 40 mins
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              ## Complete Spark SQL Application
              ### (From CSV to Parquet)

```scala
// no master URL hard-coded!
// Use spark-submit to specify it at submission time
val spark: SparkSession = SparkSession.builder.getOrCreate()
try {
  spark.read
    .option("header", true)
    .csv("inventory/*.csv")
    .write
    .mode("overwrite")
    .save("processed")
} finally {
  spark.stop()
}
```
            </script>
          </section>
          <section id="exercise-csv" data-markdown>
            <script type="text/template">
              ## Exercise: Using CSV As Data Source

              1. Write Spark SQL code
                * **spark.read.csv**
                * Use **header** and **inferSchema** options
                * Use **args** for the path to CSV file(s)
                * **show** the records
              1. Execute <b>sbt package</b>
              1. Run the application using <b>spark-submit</b>
              1. Time: 40 mins
            </script>
          </section>
          <section id="exercise-first-app-spark-standalone" data-markdown>
            <script type="text/template">
              ## Exercise: Submit Spark Application to Spark Standalone

              1. Spin up a Spark Standalone cluster
                * **bin/spark-class**
                * **org.apache.spark.deploy.master.Master**
                * **org.apache.spark.deploy.worker.Worker**
              1. Submit Spark application to Cluster
                * **spark-submit --master spark://localhost:7077**
              1. Time: 30 mins
            </script>
          </section>
        </section>

        <section>
          <section>
            <h2>spark-submit and run-example</h2>
            <ol>
              <li><b>spark-submit</b> is a shell script to manage Spark applications.</li>
              <li><b>run-example</b> is a shell script to run Spark Examples</li>
              <li>Switch to Mastering Apache Spark 2
                <ul>
                  <li><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-submit.html">Spark Submit — spark-submit shell script</a></li>
                </ul>
              </li>
            </ol>
          </section>
          <section>
            <h2>Exercise: run-example SparkPi</h2>
            <ol>
              <li>Start <b>Spark Standalone</b> cluster manager</li>
              <li><a href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples">Spark / Scala Examples distributed with Spark</a></li>
              <li>Run a Spark application using <b>run-example</b></li>
              <li>Time: 30 mins</li>
            </ol>
          </section>
        </section>

        <section>
          <section id="jdbc-exercises">
            <h1>Spark SQL + JDBC</h1>
            <h2>Exercises</h2>
          </section>
          <section id="exercise-jdbc-postgresql">
            <h2>Exercise: Working with Datasets Using JDBC (and PostgreSQL)</h2>
            <ol>
              <li>Develop a standalone Spark SQL application (IntelliJ IDEA)</li>
              <li><b>spark-submit --packages</b> to submit Spark application with PostgreSQL JDBC Driver</li>
              <li>(optionally) Use <b>Spark Standalone</b> cluster
                <ul>
                  <li>Troubleshoot missing PostgreSQL JDBC Driver jar</li>
                </ul>
              </li>
              <li>Time: 50 mins</li>
              <li>Use Mastering Apache Spark 2
                <ul>
                  <li><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/exercises/spark-exercise-dataframe-jdbc-postgresql.html">Working with Datasets using JDBC (and PostgreSQL)</a></li>
                </ul>
              </li>
            </ol>
          </section>
          <section id="exercise-table-query-cmd" data-markdown>
            <script type="text/template">
            ## Exercise: Specifying Table and SQL Query on Command Line

            1. Build up on previous exercise
            1. Use **args** for the table name and the SQL query
              * **args(0)** is the table name
              * Use **mkString** with the remining **args** for query
            1. Register **jdbc** Dataset as temporary view
              * Use **createOrReplaceTempView**
            1. Execute the SQL query using **spark.sql**
            </script>
          </section>
        </section>

        <section id="questions" style="text-align: left" data-markdown>
          <script type="text/template">
            # Questions?

              * Read [Mastering Apache Spark 2](https://bit.ly/mastering-apache-spark)
                * [https://bit.ly/mastering-apache-spark](https://bit.ly/mastering-apache-spark)
              * Follow [@jaceklaskowski](https://twitter.com/jaceklaskowski) on twitter
              * Upvote [my questions and answers on StackOverflow](http://stackoverflow.com/users/1305344/jacek-laskowski)
              * Use [Jacek's code at GitHub](https://github.com/jaceklaskowski)
              * Read [blog posts on Medium](https://medium.com/@jaceklaskowski)
              * Upvote [my answers on Quora](https://www.quora.com/profile/Jacek-Laskowski)
              * Connect on [LinkedIn](https://www.linkedin.com/in/jaceklaskowski/)
              * Visit [Jacek Laskowski's blog](https://blog.jaceklaskowski.pl)
          </script>
        </section>

      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>

        // Full list of configuration options available at:
        // https://github.com/hakimel/reveal.js#configuration
        Reveal.initialize({
            controls: true,
            progress: true,
            history: true,
            center: true,

            transition: 'slide', // none/fade/slide/convex/concave/zoom

            // Optional reveal.js plugins
            dependencies: [
                { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
                { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
                { src: 'plugin/zoom-js/zoom.js', async: true },
                { src: 'plugin/notes/notes.js', async: true }
            ]
        });

    </script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-45999426-3', 'auto');
      ga('send', 'pageview');

    </script>
  </body>
</html>
