<!doctype html>
<!-- Inspired by view-source:https://brookewenig.github.io/SparkOverview.html  -->
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <meta name="description" content="Apache Spark 2 Workshop | Structured Streaming">
    <meta name="author" content="Jacek Laskowski">

    <title>Apache Spark 2 Workshop | Structured Streaming</title>

    <link rel="stylesheet" href="reveal.js/css/reveal.css">
    <link rel="stylesheet" href="revealjs-css/jacek.css">
		<link rel="stylesheet" href="reveal.js/css/theme/beige.css">

    <!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
  </head>

  <body>
    <div class="reveal">
      <div class="slides">

        <section class="intro" data-transition="zoom" id="home">
          <span class="menu-title" style="display: none">Home</span>
          <p>
            <img width="5%" style="background:none; border:none; box-shadow:none;" data-src="images/scala-logo.png">
            <img width="17%" style="background:none; border:none; box-shadow:none;" data-src="images/spark-logo.png">
            <img width="8%" src="images/jacek_laskowski_20141201_512px.png" style="border: 0">
          </p>
          <h1 style="font-size: 3.55em;">Structured Streaming</h1>
          <h1 style="font-size: 2.77em;">Apache Spark 2.2</h1>

          <h4><a href="https://twitter.com/jaceklaskowski">@jaceklaskowski</a> / <a href="http://stackoverflow.com/users/1305344/jacek-laskowski">StackOverflow</a> / <a href="https://github.com/jaceklaskowski">GitHub</a><br />
          Books: <a href="http://bit.ly/mastering-apache-spark">Mastering Apache Spark</a>  / <a href="http://bit.ly/spark-structured-streaming">Spark Structured Streaming</a></h4>

          <footer style="font-size: small;">&copy;<a href="https://medium.com/@jaceklaskowski">Jacek Laskowski</a> 2017 / @jaceklaskowski / jacek@japila.pl</footer>
        </section>

        <section id="agenda" data-markdown>
          <script type="text/template">
            ## Agenda

            1. [Structured Streaming](#/intro)
            1. [DataStreamReader](#/datastreamreader)
            1. [DataStreamWriter](#/datastreamwriter)
            1. [Streaming Source](#/streaming-source)
            1. [Streaming Sink](#/streaming-sink)
            1. [Streaming Query](#/streaming-query)
            1. [StreamingQueryManager](#/streaming-query-manager)
            1. [Demo: Streaming with Rate Source](#/demo-rate)
            1. [Exercises](#/exercises)
          </script>
        </section>

        <section id="intro">
          <h2>Structured Streaming</h2>
          <ol>
            <li><b>Structured Streaming</b> is a new computation model as of Spark 2.0 that is an attempt to unify streaming, interactive, and batch query execution engines</li>
            <li>Structured Streaming is a <b>stream processing engine</b> with a high-level declarative streaming API built on top of Spark SQL allowing for continuous incremental execution of a structured query.</li>
            <li>Switch to Spark Structured Streaming gitbook
              <ul>
                <li><a href="https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-structured-streaming.html">Spark Structured Streaming &mdash; Streaming Datasets</a></li>
              </ul>
            </li>
          </ol>
        </section>

        <section id="datastreamreader">
          <h2>DataStreamReader</h2>
          <ol>
            <li><b>DataStreamReader</b> is an interface for executing structured queries periodically (giving you nearly-streaming processing)
              <pre style="margin-left: 0px;"><code style="width: 900px; padding-left: 50px;">
    val streamReader: DataStreamReader = spark.readStream
    // source + options
    val dataset: DataFrame = streamReader.load
              </code></pre>
            </li>
            <li>Structured query is described using Dataset API</li>
            <li>Switch to Spark Structured Streaming gitbook
              <ul>
                <li><a href="https://jaceklaskowski.gitbooks.io/spark-structured-streaming/content/spark-sql-streaming-DataStreamReader.html">DataStreamReader</a></li>
              </ul>
            </li>
          </ol>
        </section>

        <section>
          <section id="datastreamwriter">
            <h2>DataStreamWriter</h2>
            <ol>
              <li><b>DataStreamWriter</b> is the interface for writing result of streaming queries
                <pre style="margin-left: 0px;"><code style="width: 900px; padding-left: 50px;">
   val dataset: DataFrame = ...
   val streamWriter: DataStreamWriter = dataset.writeStream
                </code></pre>
              </li>
              <li>Switch to Spark Structured Streaming gitbook
                <ul>
                  <li><a href="https://jaceklaskowski.gitbooks.io/spark-structured-streaming/content/spark-sql-streaming-DataStreamWriter.html">DataStreamWriter</a></li>
                </ul>
              </li>
            </ol>
          </section>
          <section id="datastreamwriter-queryName" style="font-size: 85%">
            <h2>DataStreamWriter and Query Name</h2>
            <ol>
              <li><b>queryName</b> specifies the name of a streaming query</li>
              <pre style="margin-left: 0px;"><code style="width: 900px; padding-left: 50px;">
queryName(queryName: String): DataStreamWriter[T]
              </code></pre>
              <li>The name must be unique among all the currently active queries in the associated SparkSession</li>
            </ol>
          </section>
          <section id="datastreamwriter-outputMode" style="font-size: 85%">
            <h2>DataStreamWriter and Output Mode</h2>
            <ol>
              <li><b>Output mode</b> specifies how the result of a streaming query is written to a sink</li>
              <pre style="margin-left: 0px;"><code style="width: 900px; padding-left: 50px;">
outputMode(outputMode: String): DataStreamWriter[T]
outputMode(outputMode: OutputMode): DataStreamWriter[T] // <-- typed version
              </code></pre>
              <li><b>append</b> writes only the new rows in a streaming query</li>
              <li><b>complete</b> writes all the rows in a streaming query every time there are some updates</li>
              <li><b>update</b> writes only the rows that were updated in a streaming query every time there are some updates
                <ul>
                  <li>Equivalent to append mode if the query doesn't use aggregations</li>
                </ul>
              </li>
            </ol>
          </section>
          <section id="datastreamwriter-trigger" style="font-size: 85%">
            <h2>Setting Trigger</h2>
            <ol>
              <li><b>trigger</b> sets how often a streaming query is requested for a result</li>
              <pre style="margin-left: 0px;"><code style="width: 900px; padding-left: 50px;">
trigger(trigger: Trigger): DataStreamWriter[T]
              </code></pre>
              <li>Use <b>Trigger.ProcessingTime</b></li>
              <pre style="margin-left: 0px;"><code style="width: 900px; padding-left: 50px;">
trigger(Trigger.ProcessingTime("10 seconds"))

import scala.concurrent.duration._
trigger(Trigger.ProcessingTime(10.seconds))

import java.util.concurrent.TimeUnit
trigger(ProcessingTime.create(10, TimeUnit.SECONDS))
              </code></pre>
              <li>Defaults to <i>as fast as possible</i>, i.e. <b>Trigger.ProcessingTime(0)</b></li>
            </ol>
          </section>
          <section id="datastreamwriter-start" style="font-size: 85%">
            <h2>Starting Streaming Query</h2>
            <ol>
              <li><b>start</b> starts execution of a streaming query that will continually output results to a sink as new data arrives</li>
              <pre style="margin-left: 0px;"><code style="width: 900px; padding-left: 50px;">
start(): StreamingQuery
              </code></pre>
              <li>Returns <b>StreamingQuery</b> that can be used to interact with the streaming query</li>
              <pre style="margin-left: 0px;"><code style="width: 900px; padding-left: 50px;">
val query: StreamingQuery = counter.writeStream.start
              </code></pre>
            </ol>
          </section>
          <section id="datastreamwriter-foreach" style="font-size: 85%">
            <h2>foreach and ForeachWriter</h2>
            <ol>
              <li><b>DataStreamWriter.foreach</b> starts execution of a streaming query that will continually send results to the given <b>ForeachWriter</b> as a new data arrives</li>
              <pre style="margin-left: 0px;"><code style="width: 900px; padding-left: 50px;">
foreach(writer: ForeachWriter[T]): DataStreamWriter[T]
              </code></pre>
              <li><b>ForeachWriter</b> can be used to send the generated data to an external system</li>
              <pre style="margin-left: 0px;"><code style="width: 900px; padding-left: 50px;">
abstract class ForeachWriter[T] {
  abstract def close(errorOrNull: Throwable): Unit
  abstract def open(partitionId: Long, version: Long): Boolean
  abstract def process(value: T): Unit
}
              </code></pre>
            </ol>
          </section>
        </section>

        <section id="streaming-source">
          <h2>Streaming Source</h2>
          <ol>
            <li><b>Streaming Source</b> acts as a continuous stream of data for a streaming query</li>
            <li>Defined using <b>format</b> method on <b>DataFrameReader</b>
              <ul>
                <li>Uses <b>shortName</b> of a source</li>
              </ul>
            </li>
            <li><b>FileStreamSource</b> and <b>TextSocketSource</b>
            <li><b>KafkaSource</b> for Apache Kafka 0.10+</li>
            <li><b>RateStreamSource</b> and <b>MemoryStream</b> for unit tests, PoCs, tutorials and debugging</li>
            <li>Switch to Spark Structured Streaming
              <ul>
                <li><a href="https://jaceklaskowski.gitbooks.io/spark-structured-streaming/content/spark-sql-streaming-Source.html">Streaming Source</a></li>
              </ul>
            </li>
          </ol>
        </section>

        <section id="streaming-sink">
          <h2>Streaming Sink</h2>
          <ol>
            <li><b>Streaming Sink</b> represents an external storage to write streaming datasets to.</li>
            <li>Defined using <b>format</b> method on <b>DataFrameWriter</b>
              <ul>
                <li>Uses <b>shortName</b> of a sink</li>
              </ul>
            </li>
            <li><b>ConsoleSink</b>, <b>FileStreamSink</b> and <b>ForeachSink</b></li>
            <li><b>KafkaSink</b> for Apache Kafka 0.10+</li>
            <li><b>MemorySink</b> for unit tests, tutorials and debugging</li>
            <li>Switch to Spark Structured Streaming
              <ul>
                <li><a href="https://jaceklaskowski.gitbooks.io/spark-structured-streaming/content/spark-sql-streaming-Sink.html">Streaming Sink</a></li>
              </ul>
            </li>
          </ol>
        </section>

        <section id="streaming-query">
          <h2>StreamingQuery</h2>
          <ol>
            <li><b>StreamingQuery</b> represents a streaming query
              <pre style="margin-left: 0px;"><code style="width: 900px; padding-left: 50px;">
     val query: StreamingQuery = counter.writeStream.start
              </code></pre>
            </li>
            <li><b>id</b> is the unique id of a query</li>
            <li><b>runId</b> is the unique id of the run of a query</li>
            <li>Use <b>awaitTermination</b> to wait for the termination of a query, either by <b>query.stop</b> or by an exception</li>
            <li>Use <b>stop</b> to stop execution of a query</li>
            <li>Switch to Spark Structured Streaming
              <ul>
                <li><a href="https://jaceklaskowski.gitbooks.io/spark-structured-streaming/content/spark-sql-streaming-StreamingQuery.html">StreamingQuery</a></li>
              </ul>
            </li>
          </ol>
        </section>

        <section id="streaming-query-manager">
          <h2>StreamingQueryManager — Streaming Query Management</h2>
          <ol>
            <li><b>StreamingQueryManager</b> is the Management API for streaming queries in a <b>SparkSession</b>
              <pre style="margin-left: 0px;"><code style="width: 900px; padding-left: 50px;">
        val qm: StreamingQueryManager = spark.streams
              </code></pre>
            </li>
            <li>Switch to Spark Structured Streaming
              <ul>
                <li><a href="https://jaceklaskowski.gitbooks.io/spark-structured-streaming/content/spark-sql-streaming-StreamingQueryManager.html">StreamingQueryManager &mdash; Streaming Query Management</a></li>
              </ul>
            </li>
          </ol>
        </section>

        <section id="demo-rate" data-markdown>
          <script type="text/template">
            ## Streaming with Rate Source
            ### (Demo)

            1. Simple rate-to-console pipeline
            1. Using **rate** format for source and **console** for sink
            ```
      val q = spark
        .readStream
        .format("rate") // <-- rate source
        .option(...)
        .load
        .writeStream
        .format("console") // <-- console sink
        .option(...)
        .queryName("rate2console")
        .start
            ```
          </script>
        </section>

        <section>
          <section id="exercises">
            <div style="text-align: left">
              <h2>Exercises</h2>
              <hr>
              <p><i class="fa fa-arrow-circle-right success"></i>&nbsp;
                <a href="#/exercise-csv">Streaming CSV Datasets</a>
              </p>
              <p><i class="fa fa-arrow-circle-right success"></i>&nbsp;
                <a href="#/exercise-kafka">From and To Apache Kafka</a>
              </p>
              <p><i class="fa fa-arrow-circle-right success"></i>&nbsp;
                <a href="#/exercise-groupBy">Streaming Aggregation with groupBy</a>
              </p>
              <p><i class="fa fa-arrow-circle-right success"></i>&nbsp;
                <a href="#/exercise-foreach">foreach and ForeachWriter</a>
              </p>
            </div>
            <footer style="font-size: small;">&copy;Jacek Laskowski 2017 / @jaceklaskowski / jacek@japila.pl</footer>
          </section>
          <section id="exercise-csv" data-markdown>
            <script type="text/template">
              ## Streaming CSV Datasets

              1. Develop a standalone Spark SQL application
                * Use IntelliJ IDEA
              1. Use **csv** format for source and **console** for sink
              1. Use **sbt package** and **spark-submit**
              1. Time: **30 mins**
            </script>
          </section>
          <section id="exercise-kafka" data-markdown>
            <script type="text/template">
              ## From and To Apache Kafka

              1. Start Kafka Broker and Console Producer
              1. Develop a standalone Spark SQL application
                * Use IntelliJ IDEA
              1. (part 1) **kafka** format for source and **console** for sink
              1. (part 2) **kafka** format for source and **kafka** for sink
              1. Use **sbt package** and **spark-submit**
              1. Read [KafkaSource](https://jaceklaskowski.gitbooks.io/spark-structured-streaming/content/spark-sql-streaming-KafkaSource.html)
              1. Time: **45 mins**
            </script>
          </section>
          <section id="exercise-groupBy" data-markdown>
            <script type="text/template">
              ## Streaming Aggregation with groupBy

              1. Develop a standalone Spark SQL application
                * Use IntelliJ IDEA
              1. Read datasets from Apache Kafka (**kafka** source)
              1. Use **groupBy** operator on streaming Dataset
              1. Use **sbt package** and **spark-submit**
              1. Time: **45 mins**
            </script>
          </section>
          <section id="exercise-foreach">
            <div style="text-align: left">
              <h2>foreach and ForeachWriter</h2>
              <hr>
              <p><i class="fa fa-arrow-circle-right success"></i>&nbsp;
                Use <b>foreach</b> and <b>ForeachWriter</b>
              </p>
              <p><i class="fa fa-arrow-circle-right success"></i>&nbsp;
                Use <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter">DataStreamWriter</a> and <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.ForeachWriter">ForeachWriter</a> scaladocs
              </p>
              <p><i class="fa fa-arrow-circle-right success"></i>&nbsp;
                Time: <b>30 mins</b>
              </p>
            </div>
            <footer style="font-size: small;">&copy;Jacek Laskowski 2017 / @jaceklaskowski / jacek@japila.pl</footer>
          </section>
        </section>

        <section id="recap" data-markdown>
          <script type="text/template">
            ## Recap

            1. [Structured Streaming](#/intro)
            1. [DataStreamReader](#/datastreamreader)
            1. [DataStreamWriter](#/datastreamwriter)
            1. [Streaming Source](#/streaming-source)
            1. [Streaming Sink](#/streaming-sink)
            1. [Streaming Query](#/streaming-query)
            1. [StreamingQueryManager](#/streaming-query-manager)
            1. [Demo: Streaming with Rate Source](#/demo-rate)
            1. [Exercises](#/exercises)
          </script>
        </section>

        <section id="questions">
          <div style="text-align: left">
            <h1>Questions?</h1>
            <hr>
            <p><i class="fa fa-arrow-circle-right success"></i>&nbsp;
              Read <a href="https://bit.ly/spark-structured-streaming">Spark Structured Streaming</a> gitbook
            </p>
            <p><i class="fa fa-arrow-circle-right success"></i>&nbsp;
              Read <a href="https://bit.ly/mastering-apache-spark">Mastering Apache Spark 2</a> gitbook
            </p>
            <p><i class="fa fa-arrow-circle-right success"></i>&nbsp;
              Follow <a href="https://twitter.com/jaceklaskowski">@jaceklaskowski</a> on twitter
            </p>
            <p><i class="fa fa-arrow-circle-right success"></i>&nbsp;
              Upvote <a href="http://stackoverflow.com/users/1305344/jacek-laskowski">my questions and answers</a> on StackOverflow
            </p>
          </div>
          <footer style="font-size: small;">&copy;Jacek Laskowski 2017 / @jaceklaskowski / jacek@japila.pl</footer>
        </section>

      </div>
    </div>

    <script src="reveal.js/lib/js/head.min.js"></script>
		<script src="reveal.js/js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
        controls: true,
				progress: true,
				history: true,
				center: true,
				slideNumber: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				menu: {
					markers: true,
					openSlideNumber: true
				},
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'reveal.js/plugin/markdown/marked.js' },
					{ src: 'reveal.js/plugin/markdown/markdown.js' },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
					{ src: 'reveal.js/plugin/notes/notes.js', async: true },
          { src: 'revealjs-plugins/menu/menu.js', async: true },
					{ src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
